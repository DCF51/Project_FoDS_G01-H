# ______________________________  Code for the Project of Group G01-H ______________________________

# ==============================  Imports/Packages  ==============================
# standard packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# data split and standardizing(/scaling)
from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, GridSearchCV #je nachdem ob wir KFold brauchen, das weglassen
from sklearn.preprocessing import StandardScaler
# the 4 learning models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
    # for decision trees see: https://scikit-learn.org/stable/modules/tree.html
from sklearn.svm import SVC, NuSVC, LinearSVC
    # for svm we have different methods see: https://scikit-learn.org/stable/modules/svm.html
# metrics
from sklearn.metrics import r2_score, mean_squared_error, roc_curve, confusion_matrix, auc
from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_score, recall_score, f1_score
#Label encoder
from sklearn.preprocessing import LabelEncoder
# other
from collections import Counter


import warnings

warnings.filterwarnings('ignore')

# ==============================  Functions  ==============================

# Utility function to plot the diagonal line
def add_identity(axes, *line_args, **line_kwargs):
    identity, = axes.plot([], [], *line_args, **line_kwargs)
    def callback(axes):
        low_x, high_x = axes.get_xlim()
        low_y, high_y = axes.get_ylim()
        low = max(low_x, low_y)
        high = min(high_x, high_y)
        identity.set_data([low, high], [low, high])
    callback(axes)
    axes.callbacks.connect('xlim_changed', callback)
    axes.callbacks.connect('ylim_changed', callback)
    return axes

def evaluation_metrics(clf, y, X, ax,legend_entry='my legendEntry'):
    """
    compute multiple evaluation metrics for the provided classifier given the true labels
    and input features. Provides a plot of the roc curve on the given axis with the legend
    entry for this plot being specified, too.

    :param clf: classifier method
    :type clf: numpy array

    :param y: true class labels
    :type y: numpy array

    :param X: feature matrix
    :type X: numpy array

    :param ax: matplotlib axis to plot on
    :type legend_entry: matplotlib Axes

    :param legend_entry: the legend entry that should be displayed on the plot
    :type legend_entry: string

    :return: confusion matrix comprising the
             true positives (tp),
             true negatives  (tn),
             false positives (fp),
             and false negatives (fn)
    :rtype: four integers
    """

    # Get the label predictions
    y_test_pred    = clf.predict(X)

    # Calculate the confusion matrix given the predicted and true labels
    tn, fp, fn, tp = confusion_matrix(y, y_test_pred).ravel()
    
    print(':) Successfully implemented the confusion matrix!')

    print('Confusion matrix:\n\t  |y_true = 0\t|y_true = 1')
    print('----------|-------------|------------')
    print('y_pred = 0|  ' + str(tp) + '\t\t|' + str(fp))
    print('y_pred = 1|  ' + str(fn) + '\t\t|' + str(tn))


     # Check for denominator of zero
    def safe_divide(numerator, denominator):
        return numerator / denominator if denominator != 0 else 0

    # Calculate the evaluation metrics
    precision = safe_divide(tp, (tp + fp))
    recall = safe_divide(tp, (tp + fn))
    f1_score = safe_divide((tp), (tp + (0.5*(fn + fp))))
    specificity = safe_divide(tn, (tn + fp))
    accuracy = safe_divide((tp+tn), (tp + tn + fp + fn))

    # Get the roc curve using a sklearn function
    y_test_predict_proba  = clf.predict_proba(X)
    fp_rates, tp_rates, _ = roc_curve(y, y_test_predict_proba[:,1]) # i want the predictioin probability for the class "1"

    # Calculate the area under the roc curve using a sklearn function
    roc_auc = auc(fp_rates, tp_rates)

    # Plot on the provided axis - feel free to make this plot nicer if
    # you want to.
    ax.plot(fp_rates, tp_rates, label = 'Fold {}'.format(legend_entry))

    return [accuracy, precision, recall, specificity, f1_score, roc_auc]

# ==============================  Import data  ==============================
data = pd.read_csv(
    filepath_or_buffer='../data/healthcare-dataset-stroke-data.csv',
    index_col='id',
    dtype= {'gender':object, 'age':float, 'hypertension':bool, 'heart_disease':bool,
            'ever_married':object, 'work_type':object, 'Residence_type':object, 
            'avg_glucose_level':float, 'bmi':float, 'smoking_status':object, 'stroke':bool}
            )

# ==============================  Define datatypes  ==============================
#as categorical if needed
data['gender'] = pd.Categorical(data['gender'])
data['work_type'] = pd.Categorical(data['work_type'])
data['Residence_type'] = pd.Categorical(data['Residence_type'])
data['smoking_status'] = pd.Categorical(data['smoking_status'])

#change 'ever_married' to boolean
data['ever_married'] = data['ever_married'].map({'Yes': True, 'No': False})
data['ever_married']=data['ever_married'].astype(bool)

# ==============================  Data description  ==============================
# Shape and meaning of dataframe -- df.info(), df.shape[], df.columns, df.head()
data.info()
print('There are ', data.shape[1],'columns in the data.')
print('There are ', data.shape[0],'rows in the data.')

#data.columns not necessary because already included in data.info()

# Datatypes -- df.info() and df.dtypes
#data.dtypes not necessary because already included in data.info()

# Missing Data -- df.isna().sum()
print('missing values:')
print(data.isna().sum()) ###bmi has 201 missing values
# Calculate the mean or median of the "bmi" feature
bmi_mean = data['bmi'].mean()
bmi_median = data['bmi'].median()
print(bmi_mean)
print(bmi_median)
# Replace the missing values with the mean or median
data['bmi'].fillna(bmi_mean, inplace=True) 
print(data.isna().sum())

# Brief summary of extremes/means/medians -- df.describe()
print(data.describe())

# Check for duplicate rows -- df.duplicated()
print('sum of duplicated lines is:', data.duplicated().sum()) #marks all duplicates except the first occurence

# ==============================  Data manipulation  ==============================
# Identify the categorical (cat_cols), numerical features (num_cols) and boolean features (boolean_cols)
num_cols = ['age', 'avg_glucose_level', 'bmi']
cate_cols = ['gender', 'work_type', 'Residence_type', 'smoking_status']
boolean_cols = ['hypertension', 'heart_disease', 'ever_married', 'stroke']

# Convert boolean columns to integers (0 or 1)
data[boolean_cols] = data[boolean_cols].astype(int)

# This replaces the categorical columns with their corresponding encoded numerical values
label_encoder = LabelEncoder()
for col in cate_cols:
    data[col] = label_encoder.fit_transform(data[col])

#I dont know if the mapping is important but it could be because some machine learning algorithms can interprete data better with mapping but i am not to familiar with this point. 
for col in boolean_cols:
    unique_values = data[col].unique()
    mapping = {value: index for index, value in enumerate(unique_values)}
    data[col] = data[col].map(mapping)
    
# This helps that no more columns are created than before
columns_to_keep = num_cols + cate_cols + boolean_cols
data_e = data[columns_to_keep]


# ==============================  Data Visualization  ==============================

age_distribution = sns.histplot(x = data['age'], color = 'blue', bins=1, binwidth=1)
age_distribution.set(xlabel = 'Age', ylabel = 'Number of Patients', title = 'Age Distribution')
plt.grid(True)
age_distribution.set_axisbelow(True)
age_distribution.set_zorder(-1)

bmi_distribution = sns.histplot(x = data['bmi'], color = 'blue', bins=1, binwidth=1)
bmi_distribution.set(xlabel = 'BMI', ylabel = 'Number of Patients', title = 'BMI Distribution')
plt.grid(True)
bmi_distribution.set_axisbelow(True)
bmi_distribution.set_zorder(-1)

glucose_level_distribution = sns.histplot(x = data['avg_glucose_level'], color = 'blue', bins=1, binwidth=1)
glucose_level_distribution.set(xlabel = 'Average Glucose Level', ylabel = 'Number of Patients', title = 'Glucose Level Distribution')
plt.grid(True)
glucose_level_distribution.set_axisbelow(True)
glucose_level_distribution.set_zorder(-1)

# ==============================  Feature selection  ==============================
# don't know what exactly to use here yet

# ==============================  Data split  ==============================

X = data.copy().drop(columns='stroke')
y = data['stroke']

# ======================Normal Spkir or K-Fold?========================
class_counts = data['stroke'].value_counts()
class_proportions = data['stroke'].value_counts(normalize=True)

print("Class Counts:")
print(class_counts)

print("\nClass Proportions:")
print(class_proportions)

#So we can see there are 4861 of non-stroke and 249 with stroke. So the majority has no stroke. In the class proportions we can see the porportion of 'Flase' is 0.9513 --> 95.13% of the data. The porportion of 'True' is 0.0487 --> 4.87% of the data
==>highly imbalanced: So stratified K-Fold Cross-Valdidator would be the better option
# ––––––––––––––––––––––––––––––  SStratified K-fold cross validator  ––––––––––––––––––––––––––––––

kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
accuracy_scores = []
best_params_list = []

#Hyperparameter Tuning FIRST USE THESE AFTER DEFINE THE BEST PARAMETERS
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10],
    'max_features': ['auto','sqrt', 'log2']
}
# Majority parameter values
param_grid = {
    'max_depth': [5],
    'max_features': ['auto'],
    'min_samples_split': [2],
    'n_estimators': [100]
}

# Prepare the performance overview data frame
df_performance = pd.DataFrame(columns = ['fold','clf','accuracy','precision','recall',
                                         'specificity','F1','roc_auc'])

all_importance = pd.DataFrame(index=range(0, 5), columns=X.columns)

# Use this counter to save your performance metrics for each crossvalidation fold
# also plot the roc curve for each model and fold into a joint subplot
fold = 0
fig,axs = plt.subplots(1,2,figsize=(9, 4))

# ==============================  Models  ==============================
# Be careful that we have different names for our different dataframes (e.g. X_train_LR and X_train_RF)

for train_index, test_index in kfold.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
    # ----------------------------  Standardizing/Scaling  ---------------------------------
    sc = StandardScaler()
    #create copies like in the tutorial to avoid inplace operations
    X_train_sc, X_test_sc = X_train.copy(), X_test.copy()

    X_train_sc[num_cols] = sc.fit_transform(X_train[num_cols])
    X_test_sc[num_cols] = sc.transform(X_test[num_cols])
    
    # ––––––––––––––––––––––––––––––  Random Forest  FIRST FOR PREDICTING BEST PARAMETERS ––––––––––––––––––––––––––––––
    rf_classifier = GridSearchCV(RandomForestClassifier(random_state=1),param_grid,cv=kfold)
    
    #Fit the model
    rf_classifier.fit(X_train_sc, y_train)
   
    # Get the best parameters --> Needed for the best parameters
    best_params = rf_classifier.best_params_
    best_params_list.append(best_params)  # Store the best parameters
    # Use the best parameters to create the final model
    rf_model = RandomForestClassifier(random_state=1, **best_params)
    
    #Make prediction using the final model
    y_pred = rf_model.predict(X_test)
    
    #Calculate accuracy and store it 
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores.append(accuracy)
    
   
    # ––––––––––––––––––––––––––––––  Logistic Regression  ––––––––––––––––––––––––––––––
    log_reg_classifier = LogisticRegression()
    log_reg_classifier.fit(X_train_sc, y_train)
    
    # Make predictions
    y_pred_log_reg = log_reg_classifier.predict(X_test)
    
    # Calculate feature importance
    coefficients = log_reg_classifier.coef_
    importance = np.abs(coefficients)
    importance_scores = (importance / np.sum(importance))
    all_importance.loc[fold] = importance_scores
    
    # ––––––––––––––––––––––––––––––  Decision Tree  ––––––––––––––––––––––––––––––
    DT = DecisionTreeClassifier()

    # ––––––––––––––––––––––––––––––  Support Vector Machine  ––––––––––––––––––––––––––––––
    SVM = SVC() #or NuSVC()/ LinearSVC(), for Moreno to decide which one fits best
    
    # ––––––––––––––––––––––––––––––  Evaluate your classifiers  ––––––––––––––––––––––––––––––
    eval_metrics = evaluation_metrics(log_reg_classifier, y_test, X_test_sc, axs[0],legend_entry=str(fold))
    df_performance.loc[len(df_performance),:] = [fold,'Logistic Regression'] + eval_metrics

    eval_metrics_RF = evaluation_metrics(rf_classifier, y_test, X_test_sc, axs[1],legend_entry=str(fold))
    df_performance.loc[len(df_performance), :] = [fold, 'Random Forest'] + eval_metrics_RF
    
    # increase counter for folds
    fold += 1
    
# Edit the plot and make it nice
model_names = ['Logistic Regression','Random Forest']
for i,ax in enumerate(axs):
    ax.set_xlabel("FPR")
    ax.set_ylabel("TPR")
    add_identity(ax, color="r", ls="--",label = 'random\nclassifier')
    ax.set_title(f"ROC curve {model_names[i]}")
    ax.grid(True, linestyle = "--", linewidth = 0.5, alpha = 0.7)
    ax.legend(loc="lower right")

mean_accuracy = np.mean(accuracy_scores)
   
#===========================Best Parameter for Hyperparameter tuning ==============
#This is only needed in the beginning--> WITH THIS WE CAN TAKE THE BEST PARAMETERS FOR THE CODE ABOVE.
best_params_list

# Create a dictionary to store the majority values for each parameter
majority_values = {}

# Iterate over the parameters in the first parameter dictionary
for param in best_params_list[0]:
    # Create a list to store the parameter values
    param_values = [params[param] for params in best_params_list]
    # Count the occurrences of each value
    param_counter = Counter(param_values)
    # Find the majority value(s)
    majority_value = [value for value, count in param_counter.items() if count == max(param_counter.values())]
    # Store the majority value(s) in the dictionary
    majority_values[param] = majority_value

# Print the majority values
print("Majority Values:")
for param, value in majority_values.items():
    print(f"{param}: {value}")
#As we can see here these are the parameters which appear as the best in the majority of the folds --> So we can take these values for the k-fold crossvalidation
# ===========================  Summarize the folds  ================================
print(df_performance.groupby(by = 'clf').agg(['mean', 'std']))

display(all_importance)

# Visualize the normalized feature importance across the five folds and add error bar to indicate the std
fig, ax = plt.subplots(figsize=(18, 6))

ax.bar(np.arange(importance_scores.shape[1]), all_importance.mean(), yerr=all_importance.std())
ax.set_xticks(np.arange(importance_scores.shape[1]), X.columns.tolist(), rotation=45)
ax.set_title("Normalized feature importance for LR across 5 folds", fontsize=20)
plt.xlabel('Risk Factors', fontsize=16)
plt.ylabel("Normalized feature importance", fontsize=16)
plt.show()

# ================================ Feature importance for Random Forest =================
feature_importance = rf_model.feature_importances_

# Plot feature importance
plt.figure(figsize=(8, 6))
plt.barh(X_train.columns, feature_importance)
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('Random Forest Feature Importance')
plt.show()

# Store feature importance in the performance overview data frame
df_performance['feature_importance'] = feature_importance


# ==============================  Models  ==============================
# Be careful that we have different names for our different dataframes (e.g. X_train_LR and X_train_RF)

# ––––––––––––––––––––––––––––––  Decision Tree  ––––––––––––––––––––––––––––––
DT = DecisionTreeClassifier()

# ––––––––––––––––––––––––––––––  Support Vector Machine  ––––––––––––––––––––––––––––––
SVM = SVC() #or NuSVC()/ LinearSVC(), for Moreno to decide which one fits best
